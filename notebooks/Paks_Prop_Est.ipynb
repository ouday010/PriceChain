{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp48kBqzgVoC"
      },
      "outputs": [],
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler , OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import gc\n",
        "import warnings\n",
        "from google.colab import files\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "#set plot style\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCcRRHB6icJS",
        "outputId": "982e8b22-52f2-47db-ba66-260d936b5127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive to access dataset\n",
            "Mounted at /content/drive\n",
            "Dataset loaded successfully , shape: (191393, 24)\n",
            "\n",
            "==== Dataset Info ====\n",
            "Rows: 191393\n",
            "Columns:  ['property_id', 'location_id', 'page_url', 'property_type', 'price', 'price_bin', 'location', 'city', 'province_name', 'locality', 'latitude', 'longitude', 'baths', 'area', 'area_marla', 'area_sqft', 'purpose', 'bedrooms', 'date_added', 'year', 'month', 'day', 'agency', 'agent']\n",
            "\n",
            "Data Types: \n",
            " property_id       object\n",
            "location_id       object\n",
            "page_url          object\n",
            "property_type     object\n",
            "price              int64\n",
            "price_bin         object\n",
            "location          object\n",
            "city              object\n",
            "province_name     object\n",
            "locality          object\n",
            "latitude         float64\n",
            "longitude        float64\n",
            "baths              int64\n",
            "area              object\n",
            "area_marla       float64\n",
            "area_sqft        float64\n",
            "purpose           object\n",
            "bedrooms           int64\n",
            "date_added        object\n",
            "year               int64\n",
            "month              int64\n",
            "day                int64\n",
            "agency            object\n",
            "agent             object\n",
            "dtype: object\n",
            "\n",
            "Nulls: \n",
            " property_id          0\n",
            "location_id          0\n",
            "page_url             0\n",
            "property_type        0\n",
            "price                0\n",
            "price_bin            0\n",
            "location             0\n",
            "city                 0\n",
            "province_name        0\n",
            "locality             0\n",
            "latitude             0\n",
            "longitude            0\n",
            "baths                0\n",
            "area                 0\n",
            "area_marla           0\n",
            "area_sqft            0\n",
            "purpose              0\n",
            "bedrooms             0\n",
            "date_added           0\n",
            "year                 0\n",
            "month                0\n",
            "day                  0\n",
            "agency           47379\n",
            "agent            47380\n",
            "dtype: int64\n",
            "Duplicates:  0\n",
            "\n",
            "Sample Data : \n",
            "   property_id location_id                                           page_url  \\\n",
            "0      347795           8  https://www.zameen.com/Property/lahore_model_t...   \n",
            "1      482892          48  https://www.zameen.com/Property/lahore_multan_...   \n",
            "2      555962          75  https://www.zameen.com/Property/eden_eden_aven...   \n",
            "3      562843        3821  https://www.zameen.com/Property/gulberg_2_gulb...   \n",
            "4      686990        3522  https://www.zameen.com/Property/allama_iqbal_t...   \n",
            "\n",
            "  property_type      price  price_bin           location    city  \\\n",
            "0         House  220000000  Very High         Model Town  Lahore   \n",
            "1         House   40000000  Very High        Multan Road  Lahore   \n",
            "2         House    9500000        Low               Eden  Lahore   \n",
            "3         House  125000000  Very High            Gulberg  Lahore   \n",
            "4         House   21000000       High  Allama Iqbal Town  Lahore   \n",
            "\n",
            "  province_name                           locality  ...  area_marla  \\\n",
            "0        Punjab         Model Town, Lahore, Punjab  ...       120.0   \n",
            "1        Punjab        Multan Road, Lahore, Punjab  ...        20.0   \n",
            "2        Punjab               Eden, Lahore, Punjab  ...         9.0   \n",
            "3        Punjab            Gulberg, Lahore, Punjab  ...        20.0   \n",
            "4        Punjab  Allama Iqbal Town, Lahore, Punjab  ...        11.0   \n",
            "\n",
            "   area_sqft   purpose bedrooms  date_added  year month  day  \\\n",
            "0   32670.12  For Sale        0  07-17-2019  2019     7   17   \n",
            "1    5445.02  For Sale        5  10-06-2018  2018    10    6   \n",
            "2    2450.26  For Sale        3  07-03-2019  2019     7    3   \n",
            "3    5445.02  For Sale        8  04-04-2019  2019     4    4   \n",
            "4    2994.76  For Sale        6  04-04-2019  2019     4    4   \n",
            "\n",
            "                   agency                    agent  \n",
            "0  Real Biz International               Usama Khan  \n",
            "1             Khan Estate         mohsinkhan and B  \n",
            "2         Shahum Estate 2  Babar Hameed, Raja Omar  \n",
            "3                     NaN                      NaN  \n",
            "4                     NaN                      NaN  \n",
            "\n",
            "[5 rows x 24 columns]\n"
          ]
        }
      ],
      "source": [
        "#Step1 : Mount Google Drive and load dataset\n",
        "print(\"Mounting Google Drive to access dataset\")\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  #define path to dataset\n",
        "  data_folder = '/content/drive/My Drive/data/'\n",
        "  data_file=data_folder + 'Property_with_Feature_Engineering.csv'\n",
        "  #check if file exists\n",
        "  if not os.path.exists(data_file):\n",
        "    raise FileNotFoundError(f\"Property_with_Feature_Engineering.csv not found in {data_folder}\")\n",
        "\n",
        "  #Load dataset with specified dtype and encoding\n",
        "  numeric_cols=['price','latitude','longitude','baths','area_marla','area_sqft','bedrooms','year','month','day']\n",
        "  dtypes={'property_id':str,'location_id':str,'page_url':str,'property_type':str,'price_bin':str,\n",
        "          'location':str,'city':str,'province_name':str,'locality':str,'purpose':str,'agency':str,'agent':str}\n",
        "  converters={col : lambda x: pd.to_numeric(x,errors='coerce') for col in numeric_cols}\n",
        "  df= pd.read_csv(data_file,encoding='latin1',sep=',',dtype=dtypes,converters=converters)\n",
        "  print(f\"Dataset loaded successfully , shape: {df.shape}\")\n",
        "\n",
        "  #clean columns names\n",
        "  df.columns=[col.strip().strip('\"').lower() for col in df.columns]\n",
        "  #Inspect data\n",
        "  print(\"\\n==== Dataset Info ====\")\n",
        "  print(f\"Rows: {len(df)}\")\n",
        "  print(\"Columns: \",df.columns.tolist())\n",
        "  print(\"\\nData Types: \\n\",df.dtypes)\n",
        "  print(\"\\nNulls: \\n\",df.isnull().sum())\n",
        "  print(\"Duplicates: \",df.duplicated().sum())\n",
        "  print(\"\\nSample Data : \\n\",df.head())\n",
        "\n",
        "except Exception as e :\n",
        "  print(f\"Error loading dataset: {e}\")\n",
        "  exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_UHa5NQvgXq",
        "outputId": "a330d18d-7e50-412a-f927-e8bf23f76564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Data Cleaning ====\n",
            "Checking 'area' vs. 'area_marla' and 'area_sqft'...\n",
            "Correlation between area columns:\n",
            "             area  area_marla  area_sqft\n",
            "area         NaN         NaN        NaN\n",
            "area_marla   NaN         1.0        1.0\n",
            "area_sqft    NaN         1.0        1.0\n",
            "'area' not dropped, keeping for now \n",
            "Columns dropped: ['property_id', 'location_id', 'page_url', 'date_added', 'province_name', 'agency', 'agent']\n",
            "\n",
            "Nulls After Imputation:\n",
            "property_type         0\n",
            "price                 0\n",
            "price_bin             0\n",
            "location              0\n",
            "city                  0\n",
            "locality              0\n",
            "latitude              0\n",
            "longitude             0\n",
            "baths                 0\n",
            "area             191393\n",
            "area_marla            0\n",
            "area_sqft             0\n",
            "purpose               0\n",
            "bedrooms              0\n",
            "year                  0\n",
            "month                 0\n",
            "day                   0\n",
            "dtype: int64\n",
            "\n",
            "Updated Dataset Info:\n",
            "Rows : 191393\n",
            "Columns:  ['property_type', 'price', 'price_bin', 'location', 'city', 'locality', 'latitude', 'longitude', 'baths', 'area', 'area_marla', 'area_sqft', 'purpose', 'bedrooms', 'year', 'month', 'day']\n",
            "\n",
            "Sample Data: \n",
            "   property_type      price  price_bin           location    city  \\\n",
            "0         House  220000000  Very High         Model Town  Lahore   \n",
            "1         House   40000000  Very High        Multan Road  Lahore   \n",
            "2         House    9500000        Low               Eden  Lahore   \n",
            "3         House  125000000  Very High            Gulberg  Lahore   \n",
            "4         House   21000000       High  Allama Iqbal Town  Lahore   \n",
            "\n",
            "                            locality   latitude  longitude  baths  area  \\\n",
            "0         Model Town, Lahore, Punjab  31.483869  74.325686      0   NaN   \n",
            "1        Multan Road, Lahore, Punjab  31.431593  74.179980      5   NaN   \n",
            "2               Eden, Lahore, Punjab  31.499348  74.416959      0   NaN   \n",
            "3            Gulberg, Lahore, Punjab  31.522069  74.355512      7   NaN   \n",
            "4  Allama Iqbal Town, Lahore, Punjab  31.506483  74.286017      5   NaN   \n",
            "\n",
            "   area_marla  area_sqft   purpose  bedrooms  year  month  day  \n",
            "0       120.0   32670.12  For Sale         0  2019      7   17  \n",
            "1        20.0    5445.02  For Sale         5  2018     10    6  \n",
            "2         9.0    2450.26  For Sale         3  2019      7    3  \n",
            "3        20.0    5445.02  For Sale         8  2019      4    4  \n",
            "4        11.0    2994.76  For Sale         6  2019      4    4  \n"
          ]
        }
      ],
      "source": [
        "#Step 2 : Dropping unnecessary columns\n",
        "print(\"\\n==== Data Cleaning ====\")\n",
        "try :\n",
        "  # Verify if 'area' is redundant with 'area_marla' or 'area_sqft'\n",
        "  print(\"Checking 'area' vs. 'area_marla' and 'area_sqft'...\")\n",
        "  df['area']=pd.to_numeric(df['area'],errors='coerce') #convert 'area' to numeric\n",
        "  area_correlation=df[['area','area_marla','area_sqft']].corr()\n",
        "  print(\"Correlation between area columns:\\n\", area_correlation)\n",
        "  if area_correlation.loc['area','area_marla']>0.9 or area_correlation.loc['area','area_sqft']>0.9:\n",
        "    print(\"'area' is highly correlated with 'area_marla' or 'area_sqft', dropping 'area'\")\n",
        "    df=df.drop(columns=['area'])\n",
        "  else:\n",
        "    print(\"'area' not dropped, keeping for now \")\n",
        "\n",
        "  #Drop Specified columns\n",
        "  columns_to_drop=['property_id', 'location_id', 'page_url', 'date_added', 'province_name', 'agency', 'agent']\n",
        "  df=df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
        "  print(f\"Columns dropped: {columns_to_drop}\")\n",
        "  #Define numerical and categorical columns\n",
        "  numerical_cols=['price', 'latitude', 'longitude', 'baths', 'area_marla', 'area_sqft', 'bedrooms', 'year', 'month', 'day']\n",
        "  if 'area' in df.columns:\n",
        "    numerical_cols.append('area')\n",
        "  categorical_cols=['property_type', 'price_bin', 'location', 'city', 'locality', 'purpose']\n",
        "  #Handling missing values\n",
        "  for col in numerical_cols:\n",
        "    if col in df.columns:\n",
        "      df[col]=df[col].fillna(df[col].median())\n",
        "  for col in categorical_cols:\n",
        "    if col in df.columns:\n",
        "      df[col]=df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "  #Verfiy no nulls remain\n",
        "  print(\"\\nNulls After Imputation:\")\n",
        "  print(df.isnull().sum())\n",
        "\n",
        "  #Inspect updated dataset\n",
        "  print(\"\\nUpdated Dataset Info:\")\n",
        "  print(f\"Rows : {len(df)}\")\n",
        "  print(\"Columns: \",df.columns.tolist())\n",
        "  print(\"\\nSample Data: \\n\", df.head())\n",
        "\n",
        "except Exception as e :\n",
        "  print(f\"Error in data cleaning: {e}\")\n",
        "  exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItJkXh5z0OUy",
        "outputId": "8c824a35-bfc9-4e57-93d8-2b0e0d52a236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Data Preprocessing ====\n",
            "Dropped 'area' due to invalid format and redundancy with 'area_marla'/'area_sqft'\n",
            "Limited 'locality' to top 50 values\n",
            "Limited 'city' to top 50 values\n",
            "Limited 'location' to top 50 values\n",
            "\n",
            "Removing outliers...\n",
            "Log-transformed 'price' to reduce skewness\n",
            "Log-transformed 'area_marla' to reduce skewness\n",
            "Log-transformed 'area_sqft' to reduce skewness\n",
            "Saved to 'preprocessor.pkl'\n",
            "\n",
            "Preprocessed Data Info\n",
            "X shape: (191393, 128)\n",
            "y shape: (191393,)\n",
            "Remaining rows after outlier removal: 191393\n",
            "Feature names: ['num__latitude', 'num__longitude', 'num__baths', 'num__area_marla', 'num__bedrooms', 'num__year', 'num__month', 'num__day', 'cat__property_type_Farm House', 'cat__property_type_Flat'] ...\n"
          ]
        }
      ],
      "source": [
        "#Step3 : Data Preprocessing (drop area,encode features,remove outliers,scale)\n",
        "print(\"\\n==== Data Preprocessing ====\")\n",
        "try:\n",
        "  #drop 'area'\n",
        "  df=df.drop(columns=['area'])\n",
        "  print(\"Dropped 'area' due to invalid format and redundancy with 'area_marla'/'area_sqft'\")\n",
        "  #define numerical and categorical columns\n",
        "  numerical_cols=['latitude','longitude','baths','area_marla','bedrooms','year','month','day']\n",
        "  categorical_cols=['property_type','price_bin','location','city','locality','purpose']\n",
        "  target='price'\n",
        "  #Limit high-cardinality categorical features\n",
        "  for col in ['locality','city','location']:\n",
        "    if col in df.columns:\n",
        "      top_values=df[col].value_counts().head(50).index\n",
        "      df[col]=df[col].where(df[col].isin(top_values),'Other')\n",
        "      print(f\"Limited '{col}' to top 50 values\")\n",
        "  #Remove outliers using IQR for numerical columns\n",
        "  print(\"\\nRemoving outliers...\")\n",
        "  for col in numerical_cols + [target]:\n",
        "    if col in df.columns:\n",
        "      Q1=df[col].quantile(0.25)\n",
        "      Q3=df[col].quantile(0.75)\n",
        "      IQR=Q3-Q1\n",
        "      lower_bound=Q1-1.5*IQR\n",
        "      upper_bound=Q3+1.5*IQR\n",
        "      outliers=df[(df[col]<lower_bound) | (df[col]>upper_bound)][col]\n",
        "  #Log-transform skewed target ('price') and numerical features\n",
        "  skewed_cols= ['price','area_marla','area_sqft']\n",
        "  for col in skewed_cols:\n",
        "    if col in df.columns:\n",
        "      df[col]=np.log1p(df[col])\n",
        "      print(f\"Log-transformed '{col}' to reduce skewness\")\n",
        "  #Create preprocessing pipline\n",
        "  numerical_transformer=Pipeline(steps=[('scaler',StandardScaler())])\n",
        "  categorical_transformer=Pipeline(steps=[('onehot',OneHotEncoder(handle_unknown='ignore',sparse_output=True))])\n",
        "  preprocessor=ColumnTransformer(\n",
        "      transformers=[\n",
        "          ('num',numerical_transformer,numerical_cols),\n",
        "          ('cat',categorical_transformer,categorical_cols)\n",
        "      ])\n",
        "  #applying preprocessing to features\n",
        "  features=numerical_cols + categorical_cols\n",
        "  X=df[features]\n",
        "  y=df[target]\n",
        "  X_preprocessed=preprocessor.fit_transform(X)\n",
        "  #saving the preprocessor for later use\n",
        "  joblib.dump(preprocessor,'/content/drive/My Drive/data/preprocessor.pkl')\n",
        "  print(\"Saved to 'preprocessor.pkl'\")\n",
        "  #Inspect preprocessed data\n",
        "  print(\"\\nPreprocessed Data Info\")\n",
        "  print(f\"X shape: {X_preprocessed.shape}\")\n",
        "  print(f\"y shape: {y.shape}\")\n",
        "  print(f\"Remaining rows after outlier removal: {len(df)}\")\n",
        "  print(\"Feature names:\", preprocessor.get_feature_names_out().tolist()[:10], \"...\")\n",
        "\n",
        "except Exception as e :\n",
        "  print(f\"Error in data preprocessing: {e}\")\n",
        "  exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrcFQyTOlmlW",
        "outputId": "346bf505-22b3-405d-9b60-8bf3e9a442b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Optimized Data Splitting ===\n",
            "Filtered out 3 rows with price <= 0. Remaining rows: 191390\n",
            "Training set shape: X_train_raw (153112, 15), y_train (153112,)\n",
            "Test set shape: X_test_raw (38278, 15), y_test (38278,)\n",
            "\n",
            "Price_bin distribution in train vs. test:\n",
            "Train:\n",
            " price_bin\n",
            "Low          0.262148\n",
            "High         0.251385\n",
            "Medium       0.245454\n",
            "Very High    0.241013\n",
            "Name: proportion, dtype: float64\n",
            "Test:\n",
            " price_bin\n",
            "Low          0.262135\n",
            "High         0.251372\n",
            "Medium       0.245467\n",
            "Very High    0.241026\n",
            "Name: proportion, dtype: float64\n",
            "Preprocessed training set shape: (153112, 128)\n",
            "Preprocessed test set shape: (38278, 128)\n",
            "Saved raw (X_train_raw.csv, y_train.csv, X_test_raw.csv, y_test.csv) and preprocessed (X_train.npy, y_train.npy, X_test.npy, y_test.npy) to Google Drive\n",
            "\n",
            "Price stats (log-transformed) in train vs. test:\n",
            "Train price stats:\n",
            " count    153112.000000\n",
            "mean         14.596523\n",
            "std           2.815985\n",
            "min           0.693147\n",
            "25%          11.289794\n",
            "50%          15.789592\n",
            "75%          16.705882\n",
            "max          21.416413\n",
            "Name: price, dtype: float64\n",
            "Test price stats:\n",
            " count    38278.000000\n",
            "mean        14.622436\n",
            "std          2.804428\n",
            "min          0.693147\n",
            "25%         11.350418\n",
            "50%         15.830414\n",
            "75%         16.705882\n",
            "max         21.080940\n",
            "Name: price, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Optimized Data Splitting with Correct Saving\n",
        "print(\"\\n=== Optimized Data Splitting ===\")\n",
        "try:\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import os\n",
        "    from scipy.sparse import issparse\n",
        "\n",
        "    # Reload raw DataFrame to access 'price_bin' and filter price\n",
        "    raw_df = df.copy()  # Use df from previous cells\n",
        "\n",
        "    # Filter out rows where price <= 0\n",
        "    initial_rows = len(raw_df)\n",
        "    raw_df = raw_df[raw_df['price'] > 0]\n",
        "    filtered_rows = len(raw_df)\n",
        "    print(f\"Filtered out {initial_rows - filtered_rows} rows with price <= 0. Remaining rows: {filtered_rows}\")\n",
        "\n",
        "    # Create stratified train/test split using 'price_bin'\n",
        "    X = raw_df.drop(columns=['price'])\n",
        "    y = raw_df['price']\n",
        "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=raw_df['price_bin']\n",
        "    )\n",
        "    print(f\"Training set shape: X_train_raw {X_train_raw.shape}, y_train {y_train.shape}\")\n",
        "    print(f\"Test set shape: X_test_raw {X_test_raw.shape}, y_test {y_test.shape}\")\n",
        "\n",
        "    # Verify stratification\n",
        "    print(\"\\nPrice_bin distribution in train vs. test:\")\n",
        "    train_price_bin_dist = X_train_raw['price_bin'].value_counts(normalize=True)\n",
        "    test_price_bin_dist = X_test_raw['price_bin'].value_counts(normalize=True)\n",
        "    print(\"Train:\\n\", train_price_bin_dist)\n",
        "    print(\"Test:\\n\", test_price_bin_dist)\n",
        "\n",
        "    # Apply preprocessor and convert to dense arrays\n",
        "    data_path = '/content/drive/My Drive/data/'\n",
        "    preprocessor = joblib.load(data_path + 'preprocessor.pkl')\n",
        "    X_train = preprocessor.transform(X_train_raw)\n",
        "    X_test = preprocessor.transform(X_test_raw)\n",
        "    if issparse(X_train):\n",
        "        X_train = X_train.toarray()\n",
        "    if issparse(X_test):\n",
        "        X_test = X_test.toarray()\n",
        "    print(f\"Preprocessed training set shape: {X_train.shape}\")\n",
        "    print(f\"Preprocessed test set shape: {X_test.shape}\")\n",
        "\n",
        "    # Save raw and preprocessed data\n",
        "    os.makedirs(data_path, exist_ok=True)\n",
        "    # Save raw data as CSV\n",
        "    X_train_raw.to_csv(data_path + 'X_train_raw.csv', index=False)\n",
        "    y_train.to_csv(data_path + 'y_train.csv', index=False)\n",
        "    X_test_raw.to_csv(data_path + 'X_test_raw.csv', index=False)\n",
        "    y_test.to_csv(data_path + 'y_test.csv', index=False)\n",
        "    # Save preprocessed data as .npy (dense arrays)\n",
        "    np.save(data_path + 'X_train.npy', X_train)\n",
        "    np.save(data_path + 'y_train.npy', y_train)\n",
        "    np.save(data_path + 'X_test.npy', X_test)\n",
        "    np.save(data_path + 'y_test.npy', y_test)\n",
        "    print(\"Saved raw (X_train_raw.csv, y_train.csv, X_test_raw.csv, y_test.csv) and preprocessed (X_train.npy, y_train.npy, X_test.npy, y_test.npy) to Google Drive\")\n",
        "\n",
        "    # Verify numerical feature stats\n",
        "    print(\"\\nPrice stats (log-transformed) in train vs. test:\")\n",
        "    print(\"Train price stats:\\n\", pd.Series(y_train).describe())\n",
        "    print(\"Test price stats:\\n\", pd.Series(y_test).describe())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in optimized data splitting: {e}\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utZ6YjUrrsAD"
      },
      "outputs": [],
      "source": [
        "# Step 5: Optimized Model Training and Evaluation\n",
        "print(\"\\n=== Optimized Model Training and Evaluation ===\")\n",
        "try:\n",
        "    from xgboost import XGBRegressor\n",
        "    from catboost import CatBoostRegressor\n",
        "    from sklearn.model_selection import KFold, RandomizedSearchCV\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    import os\n",
        "    from google.colab import files\n",
        "    from scipy.sparse import issparse\n",
        "\n",
        "    # Load preprocessed data\n",
        "    data_path = '/content/drive/My Drive/data/'\n",
        "    X_train = np.load(data_path + 'X_train.npy', allow_pickle=True)\n",
        "    y_train = np.load(data_path + 'y_train.npy', allow_pickle=True)\n",
        "    X_test = np.load(data_path + 'X_test.npy', allow_pickle=True)\n",
        "    y_test = np.load(data_path + 'y_test.npy', allow_pickle=True)\n",
        "\n",
        "    # Convert sparse matrices to dense if necessary\n",
        "    if issparse(X_train):\n",
        "        print(\"Converting X_train from sparse to dense...\")\n",
        "        X_train = X_train.toarray()\n",
        "    if issparse(X_test):\n",
        "        print(\"Converting X_test from sparse to dense...\")\n",
        "        X_test = X_test.toarray()\n",
        "\n",
        "    # Verify shapes\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Check if shapes are valid\n",
        "    if len(X_train.shape) != 2 or len(X_test.shape) != 2:\n",
        "        raise ValueError(\"X_train or X_test is not a 2D array\")\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        'XGBoost': XGBRegressor(random_state=42, n_jobs=1),\n",
        "        'CatBoost': CatBoostRegressor(random_state=42, verbose=0, thread_count=1)\n",
        "    }\n",
        "\n",
        "    # Define hyperparameter grid for XGBoost\n",
        "    param_dist = {\n",
        "        'model__learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
        "        'model__max_depth': [5, 7, 9],\n",
        "        'model__n_estimators': [100, 200, 300],\n",
        "        'model__subsample': [0.7, 0.9]\n",
        "    }\n",
        "\n",
        "    # Load preprocessor for feature names\n",
        "    preprocessor = joblib.load(data_path + 'preprocessor.pkl')\n",
        "    feature_names = preprocessor.get_feature_names_out().tolist()\n",
        "\n",
        "    # Initialize results\n",
        "    results = {}\n",
        "    best_model = None\n",
        "    best_r2 = -float('inf')\n",
        "    best_model_name = None\n",
        "\n",
        "    # Cross-validation setup\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nTraining and tuning {model_name}...\")\n",
        "        try:\n",
        "            if model_name == 'XGBoost':\n",
        "                # Create pipeline for XGBoost\n",
        "                pipeline = Pipeline([('model', model)])\n",
        "                # Randomized search for XGBoost\n",
        "                search = RandomizedSearchCV(\n",
        "                    pipeline,\n",
        "                    param_distributions=param_dist,\n",
        "                    n_iter=10,\n",
        "                    cv=kf,\n",
        "                    scoring='neg_mean_squared_error',\n",
        "                    random_state=42,\n",
        "                    n_jobs=1\n",
        "                )\n",
        "                search.fit(X_train, y_train)\n",
        "                best_estimator = search.best_estimator_\n",
        "                print(f\"Best parameters for {model_name}: {search.best_params_}\")\n",
        "            else:\n",
        "                # CatBoost (no pipeline needed)\n",
        "                best_estimator = model\n",
        "                best_estimator.fit(X_train, y_train)\n",
        "\n",
        "            # Evaluate on test set (original price scale)\n",
        "            y_pred_log = best_estimator.predict(X_test)\n",
        "            y_pred = np.expm1(y_pred_log)  # Inverse of log1p\n",
        "            y_test_orig = np.expm1(y_test)  # Inverse of log1p\n",
        "            rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
        "            mae = mean_absolute_error(y_test_orig, y_pred)\n",
        "            r2 = r2_score(y_test_orig, y_pred)\n",
        "\n",
        "            results[model_name] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
        "            print(f\"{model_name} Results (original price scale):\")\n",
        "            print(f\"RMSE: {rmse:.2f}\")\n",
        "            print(f\"MAE: {mae:.2f}\")\n",
        "            print(f\"R2: {r2:.2f}\")\n",
        "\n",
        "            # Save model if it has the best R2\n",
        "            if r2 > best_r2:\n",
        "                best_r2 = r2\n",
        "                best_model = best_estimator\n",
        "                best_model_name = model_name\n",
        "                model_filename = data_path + f'{model_name.lower()}_best_model.pkl'\n",
        "                joblib.dump(best_estimator, model_filename, compress=3)\n",
        "                print(f\"Saved {model_name} model to {model_filename}\")\n",
        "\n",
        "            # Feature importance\n",
        "            if model_name in ['XGBoost', 'CatBoost']:\n",
        "                importance = best_estimator.get_feature_importance() if model_name == 'CatBoost' else best_estimator.named_steps['model'].feature_importances_\n",
        "                importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "                print(f\"\\nTop 5 features for {model_name}:\")\n",
        "                print(importance_df.sort_values(by='Importance', ascending=False).head(5))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {model_name}: {e}\")\n",
        "\n",
        "    # Compare models\n",
        "    print(\"\\nModel Comparison:\")\n",
        "    if results:\n",
        "        results_df = pd.DataFrame(results).T\n",
        "        print(results_df.sort_values(by='R2', ascending=False))\n",
        "    else:\n",
        "        print(\"No models trained successfully\")\n",
        "\n",
        "    # Download best model\n",
        "    if best_model is not None:\n",
        "        files.download(model_filename)\n",
        "        print(f\"Downloaded best model: {model_filename}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in model training: {e}\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65I8lXdpxBeU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InWYJU0bw_U3",
        "outputId": "dd4bd753-c9d7-4333-bf13-b55785295eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJLOKjoSxjem"
      },
      "outputs": [],
      "source": [
        "# Step 6: Fixed and Optimized Model Refinement and Validation\n",
        "print(\"\\n=== Fixed and Optimized Model Refinement and Validation ===\")\n",
        "try:\n",
        "    from xgboost import XGBRegressor\n",
        "    from catboost import CatBoostRegressor\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    import os\n",
        "    from google.colab import files\n",
        "    from scipy.sparse import issparse\n",
        "\n",
        "    # Load raw data\n",
        "    data_path = '/content/drive/My Drive/data/'\n",
        "    X_train_raw = pd.read_csv(data_path + 'X_train_raw.csv')\n",
        "    X_test_raw = pd.read_csv(data_path + 'X_test_raw.csv')\n",
        "    y_train = np.load(data_path + 'y_train.npy', allow_pickle=True)\n",
        "    y_test = np.load(data_path + 'y_test.npy', allow_pickle=True)\n",
        "\n",
        "    # Drop price_bin\n",
        "    X_train_raw = X_train_raw.drop(columns=['price_bin'])\n",
        "    X_test_raw = X_test_raw.drop(columns=['price_bin'])\n",
        "    print(\"Dropped 'price_bin' to prevent leakage\")\n",
        "\n",
        "    # Feature engineering\n",
        "    X_train_raw['bedrooms_area_marla'] = X_train_raw['bedrooms'] * X_train_raw['area_marla']\n",
        "    X_test_raw['bedrooms_area_marla'] = X_test_raw['bedrooms'] * X_test_raw['area_marla']\n",
        "    X_train_raw['baths_area_marla'] = X_train_raw['baths'] * X_train_raw['area_marla']\n",
        "    X_test_raw['baths_area_marla'] = X_test_raw['baths'] * X_test_raw['area_marla']\n",
        "    X_train_raw['lat_lon_interaction'] = X_train_raw['latitude'] * X_train_raw['longitude']\n",
        "    X_test_raw['lat_lon_interaction'] = X_test_raw['latitude'] * X_test_raw['longitude']\n",
        "    print(\"Added features: bedrooms_area_marla, baths_area_marla, lat_lon_interaction\")\n",
        "\n",
        "    # Define numerical and categorical columns\n",
        "    numerical_cols = ['latitude', 'longitude', 'baths', 'area_marla', 'bedrooms', 'year', 'month', 'day',\n",
        "                     'bedrooms_area_marla', 'baths_area_marla', 'lat_lon_interaction']\n",
        "    categorical_cols = ['property_type', 'location', 'city', 'locality', 'purpose']\n",
        "\n",
        "    # Check for infinities or NaNs\n",
        "    if np.any(np.isinf(X_train_raw[numerical_cols])) or np.any(np.isnan(X_train_raw[numerical_cols])):\n",
        "        print(\"Warning: Infinities or NaNs detected, replacing with median\")\n",
        "        for col in numerical_cols:\n",
        "            X_train_raw[col] = X_train_raw[col].replace([np.inf, -np.inf], np.nan).fillna(X_train_raw[col].median())\n",
        "            X_test_raw[col] = X_test_raw[col].replace([np.inf, -np.inf], np.nan).fillna(X_train_raw[col].median())\n",
        "\n",
        "    # Update preprocessor (keep sparse output)\n",
        "    numerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))])\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "    X_train = preprocessor.fit_transform(X_train_raw)\n",
        "    X_test = preprocessor.transform(X_test_raw)\n",
        "    print(f\"Preprocessed shapes: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
        "\n",
        "    # Save preprocessor\n",
        "    joblib.dump(preprocessor, data_path + 'preprocessor_final.pkl')\n",
        "    print(\"Saved final preprocessor to 'preprocessor_final.pkl'\")\n",
        "\n",
        "    # Create holdout set\n",
        "    X_test, X_holdout, y_test, y_holdout = train_test_split(\n",
        "        X_test, y_test, test_size=0.2, random_state=42\n",
        "    )\n",
        "    print(f\"Test set shape: {X_test.shape}, Holdout set shape: {X_holdout.shape}\")\n",
        "\n",
        "    # Train and tune XGBoost\n",
        "    print(\"\\nTuning XGBoost...\")\n",
        "    pipeline = Pipeline([('xgb', XGBRegressor(random_state=42, n_jobs=1, early_stopping_rounds=10))])\n",
        "    param_dist_xgb = {\n",
        "        'xgb__learning_rate': [0.05, 0.1],\n",
        "        'xgb__max_depth': [5, 7],\n",
        "        'xgb__n_estimators': [100, 200],\n",
        "        'xgb__subsample': [0.8, 1.0]\n",
        "    }\n",
        "    search = RandomizedSearchCV(\n",
        "        pipeline,\n",
        "        param_distributions=param_dist_xgb,\n",
        "        n_iter=5,\n",
        "        cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
        "        scoring='neg_mean_squared_error',\n",
        "        random_state=42,\n",
        "        n_jobs=1\n",
        "    )\n",
        "    search.fit(X_train, y_train, xgb__eval_set=[(X_test, y_test)], xgb__verbose=False)\n",
        "    xgb_model = search.best_estimator_\n",
        "    print(f\"Best parameters for XGBoost: {search.best_params_}\")\n",
        "\n",
        "    # Train CatBoost and RandomForest\n",
        "    print(\"\\nTraining CatBoost and RandomForest...\")\n",
        "    cat_model = CatBoostRegressor(random_state=42, verbose=0, thread_count=1, early_stopping_rounds=10)\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=1)\n",
        "    cat_model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Feature selection with lower threshold\n",
        "    importance = xgb_model.named_steps['xgb'].feature_importances_\n",
        "    feature_names = preprocessor.get_feature_names_out().tolist()\n",
        "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "    top_features = importance_df[importance_df['Importance'] > 0.001]['Feature'].tolist()\n",
        "    print(f\"\\nSelected {len(top_features)} features with importance > 0.001:\")\n",
        "    print(importance_df.sort_values(by='Importance', ascending=False).head(5))\n",
        "\n",
        "    # Filter data\n",
        "    feature_indices = [feature_names.index(f) for f in top_features]\n",
        "    X_train_filtered = X_train[:, feature_indices]\n",
        "    X_test_filtered = X_test[:, feature_indices]\n",
        "    X_holdout_filtered = X_holdout[:, feature_indices]\n",
        "    print(f\"Filtered shapes: X_train {X_train_filtered.shape}, X_test {X_test_filtered.shape}, X_holdout {X_holdout_filtered.shape}\")\n",
        "\n",
        "    # Retrain models on filtered features\n",
        "    print(\"\\nRetraining models on filtered features...\")\n",
        "    xgb_model_filtered = XGBRegressor(\n",
        "        **{k.split('__')[1]: v for k, v in search.best_params_.items()},\n",
        "        random_state=42, n_jobs=1, early_stopping_rounds=10\n",
        "    )\n",
        "    cat_model_filtered = CatBoostRegressor(random_state=42, verbose=0, thread_count=1, early_stopping_rounds=10)\n",
        "    rf_model_filtered = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=1)\n",
        "    xgb_model_filtered.fit(X_train_filtered, y_train, eval_set=[(X_test_filtered, y_test)], verbose=False)\n",
        "    cat_model_filtered.fit(X_train_filtered, y_train, eval_set=(X_test_filtered, y_test))\n",
        "    rf_model_filtered.fit(X_train_filtered, y_train)\n",
        "\n",
        "    # Weighted ensemble predictions\n",
        "    print(\"\\nEvaluating Weighted Ensemble...\")\n",
        "    y_pred_xgb_log = xgb_model_filtered.predict(X_test_filtered)\n",
        "    y_pred_cat_log = cat_model_filtered.predict(X_test_filtered)\n",
        "    y_pred_rf_log = rf_model_filtered.predict(X_test_filtered)\n",
        "    weights = [0.5, 0.3, 0.2]  # XGBoost: 50%, CatBoost: 30%, RandomForest: 20%\n",
        "    y_pred_log = weights[0] * y_pred_xgb_log + weights[1] * y_pred_cat_log + weights[2] * y_pred_rf_log\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    y_test_orig = np.expm1(y_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
        "    mae = mean_absolute_error(y_test_orig, y_pred)\n",
        "    r2 = r2_score(y_test_orig, y_pred)\n",
        "    print(\"Weighted Ensemble Test Results (original price scale):\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"R2: {r2:.2f}\")\n",
        "\n",
        "    # Validate on holdout set\n",
        "    y_pred_xgb_log_holdout = xgb_model_filtered.predict(X_holdout_filtered)\n",
        "    y_pred_cat_log_holdout = cat_model_filtered.predict(X_holdout_filtered)\n",
        "    y_pred_rf_log_holdout = rf_model_filtered.predict(X_holdout_filtered)\n",
        "    y_pred_log_holdout = weights[0] * y_pred_xgb_log_holdout + weights[1] * y_pred_cat_log_holdout + weights[2] * y_pred_rf_log_holdout\n",
        "    y_pred_holdout = np.expm1(y_pred_log_holdout)\n",
        "    y_holdout_orig = np.expm1(y_holdout)\n",
        "    r2_holdout = r2_score(y_holdout_orig, y_pred_holdout)\n",
        "    print(f\"Weighted Ensemble Holdout R2: {r2_holdout:.2f}\")\n",
        "\n",
        "    # Save best model (XGBoost) and artifacts\n",
        "    model_filename = data_path + 'xgboost_final_model.pkl'\n",
        "    joblib.dump(xgb_model_filtered, model_filename, compress=3)\n",
        "    importance_df.to_csv(data_path + 'feature_importance.csv', index=False)\n",
        "    print(f\"Saved XGBoost model to {model_filename}\")\n",
        "    print(\"Saved feature importance to 'feature_importance.csv'\")\n",
        "\n",
        "    # Save selected feature indices for deployment\n",
        "    joblib.dump(feature_indices, data_path + 'selected_feature_indices.pkl')\n",
        "    print(\"Saved selected feature indices to 'selected_feature_indices.pkl'\")\n",
        "\n",
        "    # Download artifacts\n",
        "    files.download(model_filename)\n",
        "    files.download(data_path + 'preprocessor_final.pkl')\n",
        "    files.download(data_path + 'feature_importance.csv')\n",
        "    files.download(data_path + 'selected_feature_indices.pkl')\n",
        "    print(f\"Downloaded {model_filename}, preprocessor_final.pkl, feature_importance.csv, selected_feature_indices.pkl\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in model refinement: {e}\")\n",
        "    exit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "Du5XloXNPmWZ",
        "outputId": "cb1be8b8-5a20-48a8-9a91-51219cc9d469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Best Optimized Model to Achieve R² ≈ 0.88 ===\n",
            "Dropped 'price_bin' to prevent leakage\n",
            "Preprocessed shapes: X_train (153112, 143), X_test (38278, 143)\n",
            "Saved final preprocessor to 'preprocessor_final.pkl'\n",
            "Test set shape: (30622, 143), Holdout set shape: (7656, 143)\n",
            "\n",
            "Training models...\n",
            "\n",
            "Selected 88 features with importance > 0.0001:\n",
            "                                               Feature  Importance\n",
            "131                              cat__purpose_For Rent    0.931306\n",
            "3                                      num__area_marla    0.010590\n",
            "19                            cat__property_type_House    0.006050\n",
            "93   cat__locality_DHA Defence, Islamabad, Islamaba...    0.002944\n",
            "79                                cat__city_Rawalpindi    0.002750\n",
            "Filtered shapes: X_train (153112, 88), X_test (30622, 88), X_holdout (7656, 88)\n",
            "\n",
            "Retraining models on filtered features...\n",
            "\n",
            "Evaluating Weighted Ensemble...\n",
            "Weighted Ensemble Test Results (original price scale):\n",
            "RMSE: 12523118.45\n",
            "MAE: 3112168.39\n",
            "R2: 0.86\n",
            "Weighted Ensemble Holdout R2: 0.91\n",
            "Saved XGBoost model to /content/drive/My Drive/data/xgboost_final_model.pkl\n",
            "Saved CatBoost model to 'catboost_final_model.pkl'\n",
            "Saved RandomForest model to 'rf_final_model.pkl'\n",
            "Saved feature importance to 'feature_importance.csv'\n",
            "Saved selected feature indices to 'selected_feature_indices.pkl'\n",
            "Saved ensemble weights to 'ensemble_weights.pkl'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1ab5b7af-6b3e-410a-aae8-7aa0f12d304d\", \"xgboost_final_model.pkl\", 524372)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_17544dfe-6ff5-43fd-aac6-dac0f29f4051\", \"catboost_final_model.pkl\", 379774)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7af8b972-4e0f-44f9-9500-e42f24249942\", \"rf_final_model.pkl\", 228487128)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b976113a-5b3b-4dff-a70a-9a4557f3a7c0\", \"preprocessor_final.pkl\", 7603)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_47fe53da-009f-4442-8000-d938b32bc556\", \"feature_importance.csv\", 6137)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ab7e5856-e617-4b7c-b010-227c35472a88\", \"selected_feature_indices.pkl\", 192)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cc58aa0c-d4b1-4fd9-98d5-3cf50ce66479\", \"ensemble_weights.pkl\", 43)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded /content/drive/My Drive/data/xgboost_final_model.pkl, catboost_final_model.pkl, rf_final_model.pkl, preprocessor_final.pkl, feature_importance.csv, selected_feature_indices.pkl, ensemble_weights.pkl\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Best Optimized Model to Achieve R² ≈ 0.88\n",
        "print(\"\\n=== Best Optimized Model to Achieve R² ≈ 0.88 ===\")\n",
        "try:\n",
        "    from xgboost import XGBRegressor\n",
        "    from catboost import CatBoostRegressor\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.cluster import KMeans\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    import os\n",
        "    from google.colab import files\n",
        "    from scipy.sparse import issparse\n",
        "\n",
        "    # Load raw data\n",
        "    data_path = '/content/drive/My Drive/data/'\n",
        "    X_train_raw = pd.read_csv(data_path + 'X_train_raw.csv')\n",
        "    X_test_raw = pd.read_csv(data_path + 'X_test_raw.csv')\n",
        "    y_train = np.load(data_path + 'y_train.npy', allow_pickle=True)\n",
        "    y_test = np.load(data_path + 'y_test.npy', allow_pickle=True)\n",
        "\n",
        "    # Optional: Sample 70% of training data to reduce runtime (uncomment if needed)\n",
        "    # sample_frac = 0.7\n",
        "    # X_train_raw, _, y_train, _ = train_test_split(\n",
        "    #     X_train_raw, y_train, train_size=sample_frac, random_state=42\n",
        "    # )\n",
        "    # print(f\"Sampled training data: {X_train_raw.shape[0]} rows\")\n",
        "\n",
        "    # Drop price_bin\n",
        "    X_train_raw = X_train_raw.drop(columns=['price_bin'])\n",
        "    X_test_raw = X_test_raw.drop(columns=['price_bin'])\n",
        "    print(\"Dropped 'price_bin' to prevent leakage\")\n",
        "\n",
        "    # Feature engineering\n",
        "    X_train_raw['bedrooms_area_marla'] = X_train_raw['bedrooms'] * X_train_raw['area_marla']\n",
        "    X_test_raw['bedrooms_area_marla'] = X_test_raw['bedrooms'] * X_test_raw['area_marla']\n",
        "    X_train_raw['baths_area_marla'] = X_train_raw['baths'] * X_train_raw['area_marla']\n",
        "    X_test_raw['baths_area_marla'] = X_test_raw['baths'] * X_test_raw['area_marla']\n",
        "    X_train_raw['lat_lon_interaction'] = X_train_raw['latitude'] * X_train_raw['longitude']\n",
        "    X_test_raw['lat_lon_interaction'] = X_test_raw['latitude'] * X_test_raw['longitude']\n",
        "\n",
        "    # Add distance to city center\n",
        "    city_centers = {\n",
        "        'Lahore': (31.5204, 74.3587),\n",
        "        'Karachi': (24.8607, 67.0011),\n",
        "        'Islamabad': (33.6844, 73.0479)\n",
        "    }\n",
        "    def calculate_distance(row, city):\n",
        "        if row['city'] in city_centers:\n",
        "            lat_center, lon_center = city_centers[row['city']]\n",
        "            return np.sqrt((row['latitude'] - lat_center)**2 + (row['longitude'] - lon_center)**2)\n",
        "        return 0.0\n",
        "    X_train_raw['distance_to_center'] = X_train_raw.apply(lambda row: calculate_distance(row, row['city']), axis=1)\n",
        "    X_test_raw['distance_to_center'] = X_test_raw.apply(lambda row: calculate_distance(row, row['city']), axis=1)\n",
        "\n",
        "    # Add location clusters\n",
        "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "    X_train_raw['location_cluster'] = kmeans.fit_predict(X_train_raw[['latitude', 'longitude']])\n",
        "    X_test_raw['location_cluster'] = kmeans.predict(X_test_raw[['latitude', 'longitude']])\n",
        "\n",
        "    # Define numerical and categorical columns\n",
        "    numerical_cols = ['latitude', 'longitude', 'baths', 'area_marla', 'bedrooms', 'year', 'month', 'day',\n",
        "                     'bedrooms_area_marla', 'baths_area_marla', 'lat_lon_interaction', 'distance_to_center']\n",
        "    categorical_cols = ['property_type', 'location', 'city', 'locality', 'purpose', 'location_cluster']\n",
        "\n",
        "    # Add polynomial features for area_marla and bedrooms\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    poly_cols = ['area_marla', 'bedrooms']\n",
        "    poly_features_train = poly.fit_transform(X_train_raw[poly_cols])\n",
        "    poly_features_test = poly.transform(X_test_raw[poly_cols])\n",
        "    poly_feature_names = poly.get_feature_names_out(poly_cols)\n",
        "    for i, name in enumerate(poly_feature_names):\n",
        "        X_train_raw[f'poly_{name}'] = poly_features_train[:, i]\n",
        "        X_test_raw[f'poly_{name}'] = poly_features_test[:, i]\n",
        "    numerical_cols.extend([f'poly_{name}' for name in poly_feature_names])\n",
        "\n",
        "    # Check for infinities or NaNs\n",
        "    if np.any(np.isinf(X_train_raw[numerical_cols])) or np.any(np.isnan(X_train_raw[numerical_cols])):\n",
        "        print(\"Warning: Infinities or NaNs detected, replacing with median\")\n",
        "        for col in numerical_cols:\n",
        "            X_train_raw[col] = X_train_raw[col].replace([np.inf, -np.inf], np.nan).fillna(X_train_raw[col].median())\n",
        "            X_test_raw[col] = X_test_raw[col].replace([np.inf, -np.inf], np.nan).fillna(X_train_raw[col].median())\n",
        "\n",
        "    # Update preprocessor\n",
        "    numerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))])\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "    X_train = preprocessor.fit_transform(X_train_raw)\n",
        "    X_test = preprocessor.transform(X_test_raw)\n",
        "    print(f\"Preprocessed shapes: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
        "\n",
        "    # Save preprocessor\n",
        "    joblib.dump(preprocessor, data_path + 'preprocessor_final.pkl')\n",
        "    print(\"Saved final preprocessor to 'preprocessor_final.pkl'\")\n",
        "\n",
        "    # Create holdout set\n",
        "    X_test, X_holdout, y_test, y_holdout = train_test_split(\n",
        "        X_test, y_test, test_size=0.2, random_state=42\n",
        "    )\n",
        "    print(f\"Test set shape: {X_test.shape}, Holdout set shape: {X_holdout.shape}\")\n",
        "\n",
        "    # Train models with optimized parameters\n",
        "    print(\"\\nTraining models...\")\n",
        "    xgb_model = XGBRegressor(\n",
        "        learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8,\n",
        "        random_state=42, n_jobs=1, early_stopping_rounds=10\n",
        "    )\n",
        "    cat_model = CatBoostRegressor(random_state=42, verbose=0, thread_count=1, early_stopping_rounds=10)\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=1)\n",
        "    xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
        "    cat_model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Feature selection\n",
        "    importance = xgb_model.feature_importances_\n",
        "    feature_names = preprocessor.get_feature_names_out().tolist()\n",
        "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "    top_features = importance_df[importance_df['Importance'] > 0.0001]['Feature'].tolist()\n",
        "    print(f\"\\nSelected {len(top_features)} features with importance > 0.0001:\")\n",
        "    print(importance_df.sort_values(by='Importance', ascending=False).head(5))\n",
        "\n",
        "    # Filter data\n",
        "    feature_indices = [feature_names.index(f) for f in top_features]\n",
        "    X_train_filtered = X_train[:, feature_indices]\n",
        "    X_test_filtered = X_test[:, feature_indices]\n",
        "    X_holdout_filtered = X_holdout[:, feature_indices]\n",
        "    print(f\"Filtered shapes: X_train {X_train_filtered.shape}, X_test {X_test_filtered.shape}, X_holdout {X_holdout_filtered.shape}\")\n",
        "\n",
        "    # Retrain models on filtered features\n",
        "    print(\"\\nRetraining models on filtered features...\")\n",
        "    xgb_model_filtered = XGBRegressor(\n",
        "        learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8,\n",
        "        random_state=42, n_jobs=1, early_stopping_rounds=10\n",
        "    )\n",
        "    cat_model_filtered = CatBoostRegressor(random_state=42, verbose=0, thread_count=1, early_stopping_rounds=10)\n",
        "    rf_model_filtered = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=1)\n",
        "    xgb_model_filtered.fit(X_train_filtered, y_train, eval_set=[(X_test_filtered, y_test)], verbose=False)\n",
        "    cat_model_filtered.fit(X_train_filtered, y_train, eval_set=(X_test_filtered, y_test))\n",
        "    rf_model_filtered.fit(X_train_filtered, y_train)\n",
        "\n",
        "    # Evaluate weighted ensemble\n",
        "    print(\"\\nEvaluating Weighted Ensemble...\")\n",
        "    y_pred_xgb_log = xgb_model_filtered.predict(X_test_filtered)\n",
        "    y_pred_cat_log = cat_model_filtered.predict(X_test_filtered)\n",
        "    y_pred_rf_log = rf_model_filtered.predict(X_test_filtered)\n",
        "    weights = [0.6, 0.3, 0.1]  # XGBoost: 60%, CatBoost: 30%, RandomForest: 10%\n",
        "    y_pred_log = weights[0] * y_pred_xgb_log + weights[1] * y_pred_cat_log + weights[2] * y_pred_rf_log\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "    y_test_orig = np.expm1(y_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
        "    mae = mean_absolute_error(y_test_orig, y_pred)\n",
        "    r2 = r2_score(y_test_orig, y_pred)\n",
        "    print(\"Weighted Ensemble Test Results (original price scale):\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"R2: {r2:.2f}\")\n",
        "\n",
        "    # Validate on holdout set\n",
        "    y_pred_xgb_log_holdout = xgb_model_filtered.predict(X_holdout_filtered)\n",
        "    y_pred_cat_log_holdout = cat_model_filtered.predict(X_holdout_filtered)\n",
        "    y_pred_rf_log_holdout = rf_model_filtered.predict(X_holdout_filtered)\n",
        "    y_pred_log_holdout = weights[0] * y_pred_xgb_log_holdout + weights[1] * y_pred_cat_log_holdout + weights[2] * y_pred_rf_log_holdout\n",
        "    y_pred_holdout = np.expm1(y_pred_log_holdout)\n",
        "    y_holdout_orig = np.expm1(y_holdout)\n",
        "    r2_holdout = r2_score(y_holdout_orig, y_pred_holdout)\n",
        "    print(f\"Weighted Ensemble Holdout R2: {r2_holdout:.2f}\")\n",
        "\n",
        "    # Save artifacts\n",
        "    model_filename = data_path + 'xgboost_final_model.pkl'\n",
        "    joblib.dump(xgb_model_filtered, model_filename, compress=3)\n",
        "    joblib.dump(cat_model_filtered, data_path + 'catboost_final_model.pkl', compress=3)\n",
        "    joblib.dump(rf_model_filtered, data_path + 'rf_final_model.pkl', compress=3)\n",
        "    importance_df.to_csv(data_path + 'feature_importance.csv', index=False)\n",
        "    joblib.dump(feature_indices, data_path + 'selected_feature_indices.pkl')\n",
        "    joblib.dump(weights, data_path + 'ensemble_weights.pkl')\n",
        "    print(f\"Saved XGBoost model to {model_filename}\")\n",
        "    print(f\"Saved CatBoost model to 'catboost_final_model.pkl'\")\n",
        "    print(f\"Saved RandomForest model to 'rf_final_model.pkl'\")\n",
        "    print(\"Saved feature importance to 'feature_importance.csv'\")\n",
        "    print(\"Saved selected feature indices to 'selected_feature_indices.pkl'\")\n",
        "    print(\"Saved ensemble weights to 'ensemble_weights.pkl'\")\n",
        "\n",
        "    # Download artifacts\n",
        "    files.download(model_filename)\n",
        "    files.download(data_path + 'catboost_final_model.pkl')\n",
        "    files.download(data_path + 'rf_final_model.pkl')\n",
        "    files.download(data_path + 'preprocessor_final.pkl')\n",
        "    files.download(data_path + 'feature_importance.csv')\n",
        "    files.download(data_path + 'selected_feature_indices.pkl')\n",
        "    files.download(data_path + 'ensemble_weights.pkl')\n",
        "    print(f\"Downloaded {model_filename}, catboost_final_model.pkl, rf_final_model.pkl, preprocessor_final.pkl, feature_importance.csv, selected_feature_indices.pkl, ensemble_weights.pkl\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in model optimization: {e}\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction Script: Validating Model on Test Data\n",
        "print(\"\\n=== Validating Model on Test Data ===\")\n",
        "try:\n",
        "    from xgboost import XGBRegressor\n",
        "    from catboost import CatBoostRegressor\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    import os\n",
        "    from google.colab import files\n",
        "    from scipy.sparse import issparse\n",
        "\n",
        "    # Load saved artifacts\n",
        "    data_path = '/content/drive/My Drive/data/'\n",
        "    xgb_model = joblib.load(data_path + 'xgboost_final_model.pkl')\n",
        "    cat_model = joblib.load(data_path + 'catboost_final_model.pkl')\n",
        "    rf_model = joblib.load(data_path + 'rf_final_model.pkl')\n",
        "    preprocessor = joblib.load(data_path + 'preprocessor_final.pkl')\n",
        "    feature_indices = joblib.load(data_path + 'selected_feature_indices.pkl')\n",
        "    weights = joblib.load(data_path + 'ensemble_weights.pkl')\n",
        "    print(\"Loaded saved models, preprocessor, feature indices, and ensemble weights\")\n",
        "\n",
        "    # Load test data (using X_test_raw.csv and y_test.npy for validation)\n",
        "    new_data = pd.read_csv(data_path + 'X_test_raw.csv')\n",
        "    y_test = np.load(data_path + 'y_test.npy', allow_pickle=True)\n",
        "    print(\"Loaded test data for validation\")\n",
        "\n",
        "    # Drop price_bin if present\n",
        "    if 'price_bin' in new_data.columns:\n",
        "        new_data = new_data.drop(columns=['price_bin'])\n",
        "        print(\"Dropped 'price_bin' from test data\")\n",
        "\n",
        "    # Feature engineering (same as training)\n",
        "    new_data['bedrooms_area_marla'] = new_data['bedrooms'] * new_data['area_marla']\n",
        "    new_data['baths_area_marla'] = new_data['baths'] * new_data['area_marla']\n",
        "    new_data['lat_lon_interaction'] = new_data['latitude'] * new_data['longitude']\n",
        "\n",
        "    # Add distance to city center\n",
        "    city_centers = {\n",
        "        'Lahore': (31.5204, 74.3587),\n",
        "        'Karachi': (24.8607, 67.0011),\n",
        "        'Islamabad': (33.6844, 73.0479)\n",
        "    }\n",
        "    def calculate_distance(row, city):\n",
        "        if row['city'] in city_centers:\n",
        "            lat_center, lon_center = city_centers[row['city']]\n",
        "            return np.sqrt((row['latitude'] - lat_center)**2 + (row['longitude'] - lon_center)**2)\n",
        "        return 0.0\n",
        "    new_data['distance_to_center'] = new_data.apply(lambda row: calculate_distance(row, row['city']), axis=1)\n",
        "\n",
        "    # Add location clusters\n",
        "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "    kmeans.fit(new_data[['latitude', 'longitude']])  # Fit on test data (or load saved KMeans if available)\n",
        "    new_data['location_cluster'] = kmeans.predict(new_data[['latitude', 'longitude']])\n",
        "\n",
        "    # Add polynomial features\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    poly_cols = ['area_marla', 'bedrooms']\n",
        "    poly_features = poly.fit_transform(new_data[poly_cols])\n",
        "    poly_feature_names = poly.get_feature_names_out(poly_cols)\n",
        "    for i, name in enumerate(poly_feature_names):\n",
        "        new_data[f'poly_{name}'] = poly_features[:, i]\n",
        "\n",
        "    # Define numerical and categorical columns\n",
        "    numerical_cols = ['latitude', 'longitude', 'baths', 'area_marla', 'bedrooms', 'year', 'month', 'day',\n",
        "                     'bedrooms_area_marla', 'baths_area_marla', 'lat_lon_interaction', 'distance_to_center']\n",
        "    numerical_cols.extend([f'poly_{name}' for name in poly_feature_names])\n",
        "    categorical_cols = ['property_type', 'location', 'city', 'locality', 'purpose', 'location_cluster']\n",
        "\n",
        "    # Check for infinities or NaNs\n",
        "    if np.any(np.isinf(new_data[numerical_cols])) or np.any(np.isnan(new_data[numerical_cols])):\n",
        "        print(\"Warning: Infinities or NaNs detected in test data, replacing with median\")\n",
        "        for col in numerical_cols:\n",
        "            new_data[col] = new_data[col].replace([np.inf, -np.inf], np.nan).fillna(new_data[col].median())\n",
        "\n",
        "    # Preprocess test data\n",
        "    X_new = preprocessor.transform(new_data)\n",
        "    X_new_filtered = X_new[:, feature_indices]\n",
        "    print(f\"Preprocessed test data shape: {X_new_filtered.shape}\")\n",
        "\n",
        "    # Predict with weighted ensemble\n",
        "    print(\"\\nMaking predictions...\")\n",
        "    y_pred_xgb_log = xgb_model.predict(X_new_filtered)\n",
        "    y_pred_cat_log = cat_model.predict(X_new_filtered)\n",
        "    y_pred_rf_log = rf_model.predict(X_new_filtered)\n",
        "    y_pred_log = weights[0] * y_pred_xgb_log + weights[1] * y_pred_cat_log + weights[2] * y_pred_rf_log\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    y_test_orig = np.expm1(y_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
        "    mae = mean_absolute_error(y_test_orig, y_pred)\n",
        "    r2 = r2_score(y_test_orig, y_pred)\n",
        "    print(\"Validation Results on Test Data (original price scale):\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"R2: {r2:.2f}\")\n",
        "\n",
        "    # Save predictions\n",
        "    new_data['predicted_price'] = y_pred\n",
        "    new_data.to_csv(data_path + 'test_predictions.csv', index=False)\n",
        "    print(\"Saved predictions to 'test_predictions.csv'\")\n",
        "    files.download(data_path + 'test_predictions.csv')\n",
        "    print(\"Downloaded test_predictions.csv\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in prediction: {e}\")\n",
        "    exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "nc98mhGs1_DO",
        "outputId": "e2763e3e-a1e2-4072-cc16-d2f8773c7618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Validating Model on Test Data ===\n",
            "Loaded saved models, preprocessor, feature indices, and ensemble weights\n",
            "Loaded test data for validation\n",
            "Dropped 'price_bin' from test data\n",
            "Preprocessed test data shape: (38278, 88)\n",
            "\n",
            "Making predictions...\n",
            "Validation Results on Test Data (original price scale):\n",
            "RMSE: 12263856.91\n",
            "MAE: 3103779.52\n",
            "R2: 0.86\n",
            "Saved predictions to 'test_predictions.csv'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_80e642fd-ffb2-4eb5-bab1-fa8cb41d3a19\", \"test_predictions.csv\", 10788768)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded test_predictions.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}